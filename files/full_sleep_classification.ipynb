{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MauroAbidalCarrer/CPP0/blob/master/files/full_sleep_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783e24cf",
      "metadata": {
        "id": "783e24cf"
      },
      "source": [
        "\n",
        "# Sleep Spindle Study\n",
        "\n",
        "## Building Model\n",
        "\n",
        "In this notebook, we build a model to detect the presence of sleep spindles in the entire EEG recording.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Ij62zkKwkMJ6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij62zkKwkMJ6",
        "outputId": "0bca9c58-ff80-4264-b02d-7e4dbe31ae7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.6/423.6 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for yasa (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyriemann (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antropy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install mne -q\n",
        "!pip install vmdpy -q\n",
        "!pip install yasa -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YlBkcCPRkoy9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlBkcCPRkoy9",
        "outputId": "16f370e9-2a61-4763-aa38-b0c5674a280d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EEG_octo'...\n",
            "remote: Enumerating objects: 311, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 311 (delta 55), reused 66 (delta 20), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (311/311), 510.53 MiB | 20.19 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "Updating files: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b full_sleep_multi_label_classification https://github.com/samymessal/EEG_octo\n",
        "import sys\n",
        "sys.path.append('/content/EEG_octo/files')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c44cdde",
      "metadata": {
        "id": "2c44cdde"
      },
      "source": [
        "\n",
        "## Imports\n",
        "\n",
        "We will import the necessary libraries that are needed for processing the data, building the model, and evaluating its performance.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31dceae2",
      "metadata": {
        "id": "31dceae2"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "import data_preparation\n",
        "import preprocess\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import json\n",
        "from tensorflow.keras.metrics import Metric\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM, Dense, BatchNormalization, Flatten, LayerNormalization\n",
        "import tensorflow.keras.layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import timeseries_dataset_from_array\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import detrend\n",
        "import yasa\n",
        "from scipy.signal import welch\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Flatten, Dense, Input, concatenate, Lambda\n",
        "from tensorflow.keras import Model, regularizers\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "DEFAULT_DIVIDER = 10000000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5642274",
      "metadata": {
        "id": "f5642274"
      },
      "source": [
        "### Download data\n",
        "\n",
        "Using the `processed_data` function from the previous step to download our concatenated raw with its correspondent preprocessing and features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "18bae3b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18bae3b7",
        "outputId": "a609cc63-d7a7-4821-bf29-00bd76784b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RawArray with float64 data, n_channels=1, n_times=4965399\n",
            "    Range : 0 ... 4965398 =      0.000 ... 19861.592 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=4965399\n",
            "    Range : 0 ... 4965398 =      0.000 ... 19861.592 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=397232\n",
            "    Range : 0 ... 397231 =      0.000 ...  1588.924 secs\n",
            "Ready.\n",
            "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RawArray with float64 data, n_channels=1, n_times=5772730\n",
            "    Range : 0 ... 5772729 =      0.000 ... 23090.916 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=5772730\n",
            "    Range : 0 ... 5772729 =      0.000 ... 23090.916 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=461818\n",
            "    Range : 0 ... 461817 =      0.000 ...  1847.268 secs\n",
            "Ready.\n",
            "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n"
          ]
        }
      ],
      "source": [
        "def load_eeg_data(mat_file_path):\n",
        "    # Load the .mat file using scipy\n",
        "    mat = loadmat(mat_file_path)\n",
        "    # Extract EEG data\n",
        "    return mat['EEG'][0, 0]['data']\n",
        "\n",
        "def mk_raw_obj(eeg_data, sfreq=250):\n",
        "    info = mne.create_info(\n",
        "        ch_names=[f'EEG{i}' for i in range(len(eeg_data))],\n",
        "        sfreq=sfreq,\n",
        "        ch_types=['eeg' for _ in range(len(eeg_data))]\n",
        "    )\n",
        "\n",
        "    return mne.io.RawArray(eeg_data, info)\n",
        "\n",
        "def load_data(file_path, labels_path):\n",
        "    raw_mat = load_eeg_data(file_path)\n",
        "    raw = mk_raw_obj(raw_mat)\n",
        "    raw_data = raw.get_data()\n",
        "    labels = pd.read_csv(labels_path)\n",
        "    labels.sort_values(\"Timestamp\", inplace=True)\n",
        "    return raw, raw_data, labels\n",
        "\n",
        "def mne_events_from_labels_df(recording_raw_obj: mne.io.Raw, labels_df: pd.DataFrame, target_label: str,):\n",
        "    presence_mask = labels_df[target_label] == 1\n",
        "    nb_events = len(labels_df[presence_mask])\n",
        "\n",
        "    return np.column_stack((labels_df['Timestamp'][presence_mask], np.ones(nb_events), np.ones(nb_events)))\n",
        "\n",
        "\n",
        "def preprocess_data(recording_data,\n",
        "                    frequency_band,\n",
        "                    sampling_freq,\n",
        "                    resampling_freq,\n",
        "                    labels_df,\n",
        "                    target_label,\n",
        "                    window_size_in_seconds):\n",
        "    # Detrending\n",
        "    recording_data = detrend(recording_data)\n",
        "\n",
        "    # Create mne raw obj\n",
        "    recording_raw_obj = mk_raw_obj(recording_data)\n",
        "    events = mne_events_from_labels_df(recording_raw_obj, labels_df, target_label)\n",
        "\n",
        "    # Resampling\n",
        "    recording_raw_obj, events = recording_raw_obj.resample(resampling_freq, events=events)\n",
        "    labels_df = pd.DataFrame(events, columns=[\"Timestamp\", \"ignore\", target_label])\n",
        "\n",
        "    # Band pass filtering\n",
        "    raw_obj = mk_raw_obj(recording_data, sfreq=sampling_freq)\n",
        "    bp_filter_raw_obj = raw_obj.filter(frequency_band[0], frequency_band[1], verbose=0)\n",
        "    recording_data = bp_filter_raw_obj.get_data()\n",
        "\n",
        "    return recording_raw_obj, labels_df\n",
        "\n",
        "def hypnogram_propas(recording_raw_obj, sampling_freq=250):\n",
        "    \"\"\"\n",
        "    Computes the propabilites of the each sleep stages at each 30s epoch.\n",
        "    Then, upsamples the probabilites to match the shape of the recording.\n",
        "    ### Parameters:\n",
        "    recording_data: ndarray of the recording\n",
        "    ### Returns:\n",
        "    Tuple of shape four, each item is a 1D array of the probability of a sleep stage at a given timestamp.\n",
        "    Four for the four sleep stages: awake, REM, NREM1, NREM2, NREM3\n",
        "    \"\"\"\n",
        "    # For some reason, yasa doesn't work properly with the unscaled data.\n",
        "    scalled_raw_obj = mk_raw_obj(recording_raw_obj.get_data() / DEFAULT_DIVIDER, sfreq=sampling_freq)\n",
        "    sls = yasa.SleepStaging(scalled_raw_obj, eeg_name=\"EEG0\")\n",
        "    hypno_proba = sls.predict_proba()\n",
        "    return [yasa.hypno_upsample_to_data(hypno_proba[column], 1/30, scalled_raw_obj, verbose=False) for column in hypno_proba.columns]\n",
        "\n",
        "def dataset_from_files(\n",
        "        recording_files,\n",
        "        labels_files,\n",
        "        target_label,\n",
        "        resampling_freq,\n",
        "        sampling_freq,\n",
        "        frequency_band,\n",
        "        window_size_in_seconds,\n",
        "        shuffle=False\n",
        "        ):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the EEG recordings.\n",
        "    Returns a dataset keras obj.\n",
        "\n",
        "    ### Parameters:\n",
        "    recording_files: List of tuples of (.mat single channel eeg_recording\n",
        "    labels_files: .csv recording labels\n",
        "    sampling_freq: sampling frequency of the recording.\n",
        "    target_label: target label\n",
        "    frequency_band: tuple (min frequency, max frequency), if not None, used to band pass filter the recordings\n",
        "\n",
        "    ### Returns:\n",
        "    (dataset, window_size)\n",
        "    dataset: Timeseries dataset keras obj\n",
        "    window_size: window size in timestamps after resampling\n",
        "    \"\"\"\n",
        "    time_series = []\n",
        "    target_timeseries = []\n",
        "    for recording_file, labels_file in zip(recording_files, labels_files):\n",
        "        # Loading\n",
        "        labels_df = pd.read_csv(labels_file)\n",
        "        recording_data = load_eeg_data(recording_file)\n",
        "\n",
        "        # Preprocessing\n",
        "        preprocessed_recording_raw_obj, labels_df = preprocess_data(\n",
        "            recording_data,\n",
        "            frequency_band,\n",
        "            sampling_freq,\n",
        "            resampling_freq,\n",
        "            labels_df,\n",
        "            target_label,\n",
        "            window_size_in_seconds\n",
        "            )\n",
        "\n",
        "        # Feature engineering\n",
        "        hypno_propas = hypnogram_propas(preprocessed_recording_raw_obj, sampling_freq=sampling_freq)\n",
        "\n",
        "        # Features formatting\n",
        "        time_serie = np.column_stack((\n",
        "            preprocessed_recording_raw_obj.get_data().squeeze(),\n",
        "            *hypno_propas,\n",
        "            ))\n",
        "        target_timeserie = np.zeros(time_serie.shape[0])\n",
        "        for timestamp in labels_df[\"Timestamp\"]:\n",
        "          target_timeserie[int(timestamp)] = 1\n",
        "        time_series.append(time_serie)\n",
        "        target_timeseries.append(target_timeserie)\n",
        "\n",
        "    concat_timeseries = np.concatenate(time_series)\n",
        "    concat_target_timeseries = np.concatenate(target_timeseries)\n",
        "\n",
        "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(concat_target_timeseries), y=concat_target_timeseries)\n",
        "    window_size = int(window_size_in_seconds * resampling_freq)\n",
        "    dataset = timeseries_dataset_from_array(\n",
        "        concat_timeseries,\n",
        "        concat_target_timeseries,\n",
        "        window_size,\n",
        "        shuffle=shuffle\n",
        "        )\n",
        "\n",
        "\n",
        "    def filter_func(input, label):\n",
        "      # Replace 'overrepresented_class' with the actual class label\n",
        "      # Replace 'keep_prob' with the probability of keeping an instance of the overrepresented class\n",
        "      return tf.cond(tf.equal(label, 0),\n",
        "                    lambda: tf.less(tf.random.uniform(shape=[], minval=0, maxval=1), 0.5),\n",
        "                    lambda: tf.constant(True))\n",
        "\n",
        "\n",
        "    # Apply the filter\n",
        "    # Unbatch the dataset if it's already batched\n",
        "    dataset = dataset.unbatch()\n",
        "\n",
        "    # Apply the filter function\n",
        "    filtered_dataset = dataset.filter(filter_func)\n",
        "\n",
        "    # Re-batch the dataset\n",
        "    batch_size = 128  # Or your desired batch size\n",
        "    filtered_dataset = filtered_dataset.batch(batch_size)\n",
        "\n",
        "\n",
        "    return dataset, window_size, class_weights\n",
        "\n",
        "\n",
        "dataset, window_size, class_weights = dataset_from_files(\n",
        "    [\"/content/EEG_octo/dataset/train_S002_night1_hackathon_raw.mat\",\n",
        "    \"/content/EEG_octo/dataset/train_S003_night5_hackathon_raw.mat\"\n",
        "    ],\n",
        "    [\"/content/EEG_octo/dataset/train_S002_labeled.csv\",\n",
        "    \"/content/EEG_octo/dataset/train_S003_labeled.csv\"\n",
        "    ],\n",
        "    sampling_freq=250,\n",
        "    resampling_freq=20,\n",
        "    target_label=\"SS1\",\n",
        "    frequency_band=(8, 16),\n",
        "    window_size_in_seconds=2.5,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa572fe2",
      "metadata": {
        "id": "aa572fe2"
      },
      "source": [
        "\n",
        "#### Model\n",
        "\n",
        "The chosen model is an LSTM, since we are dealing with timeframes, LSTM are known to deal well with time depending samples. A k-cross validation is implemented, partitioning the data into 5 parts and alterning between the 4 parts for training and the 1 for testing.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d58c9783",
      "metadata": {
        "id": "d58c9783"
      },
      "outputs": [],
      "source": [
        "class F1Score(Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "        self.f1_score = self.add_weight(name='f1', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        self.f1_score.assign(2 * ((p * r) / (p + r + tf.keras.backend.epsilon())))\n",
        "\n",
        "    def result(self):\n",
        "        return self.f1_score\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "        self.f1_score.assign(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "062e6a1a-72af-4fe7-a1cc-ae81ace437eb",
      "metadata": {
        "id": "062e6a1a-72af-4fe7-a1cc-ae81ace437eb"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    input_layer = keras.Input(shape=(window_size, 6))\n",
        "\n",
        "    # Input layer for the EEG time series\n",
        "    input_eeg = Lambda(lambda y: y[:, :, 0:2])(input_layer)\n",
        "\n",
        "    # Layer normalization for EEG\n",
        "    norm_eeg = LayerNormalization()(input_eeg)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=32, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(norm_eeg)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=64, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=128, kernel_size=5, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Now you can flatten the output if you haven't applied global pooling before\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Input layer for the other features\n",
        "    first_elements = Lambda(lambda y: y[:, 0, 2:])(input_layer)\n",
        "    # Concatenate the CNN output and the first elements of the other features\n",
        "    concatenated = concatenate([x, first_elements])\n",
        "\n",
        "    x = Dense(\n",
        "        2048, activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(concatenated)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(\n",
        "        1024,\n",
        "        activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(\n",
        "        128,\n",
        "        activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(x)\n",
        "    output_layer = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7637f27c-9443-47ce-87c3-879d60a2caa6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "7637f27c-9443-47ce-87c3-879d60a2caa6",
        "outputId": "d4d94d8c-e84e-44e2-ac9b-f44eb918124e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x: (50, 6)\n",
            "Shape of y: ()\n",
            "y: tf.Tensor(0.0, shape=(), dtype=float64)\n",
            "Shape of x: (50, 6)\n",
            "Shape of y: ()\n",
            "y: tf.Tensor(0.0, shape=(), dtype=float64)\n",
            "Shape of x: (50, 6)\n",
            "Shape of y: ()\n",
            "y: tf.Tensor(0.0, shape=(), dtype=float64)\n",
            "50\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 50, 6), found shape=(None, 6)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1053c7526b6d>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 50, 6), found shape=(None, 6)\n"
          ]
        }
      ],
      "source": [
        "for x, y in dataset.take(3):\n",
        "    print(\"Shape of x:\", x.shape)\n",
        "    print(\"Shape of y:\", y.shape)\n",
        "    print(\"y:\", y)\n",
        "print(window_size)\n",
        "\n",
        "# Define the model architecture\n",
        "model = create_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(),\n",
        "        tf.keras.metrics.Recall(),\n",
        "        F1Score(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    epochs=30,\n",
        "    class_weight={label: weight for label, weight in enumerate(class_weights)}\n",
        ")\n",
        "\n",
        "\n",
        "training_f1_scores = history.history['f1_score']\n",
        "validation_f1_scores = history.history['val_f1_score']\n",
        "\n",
        "plt.plot(training_f1_scores, label='Training F1 Score')\n",
        "plt.plot(validation_f1_scores, label='Validation F1 Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}